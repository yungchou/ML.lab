#"Neighborhood",
#"Condition1",   "BldgType",
"HouseStyle",
#"OverallQual",
"OverallCond",  "YearBuilt",
"YearRemodAdd",
#"Exterior1st",
#"Exterior2nd",
#"MasVnrType",
"ExterQual", "ExterCond",
#"BsmtQual",
#"BsmtCond",
#"BsmtExposure",
#"BsmtFinType1",
"BsmtFinSF1",
#"BsmtUnfSF",
#"TotalBsmtSF",
#"HeatingQC", "CentralAir",
"X1stFlrSF",
#"X2ndFlrSF",
"GrLivArea", "FullBath",
#"HalfBath",
#"BedroomAbvGr", "KitchenAbvGr",
"KitchenQual",
#"TotRmsAbvGrd",
"Functional",   "Fireplaces",
"GarageType",
#"GarageYrBlt",
#"GarageFinish",
"GarageCars"
#"GarageArea",
#"GarageQual",   "GarageCond"
#"PavedDrive",
#"WoodDeckSF",   "OpenPorchSF"
)
#names(train.imp)
#-------------------------------
# Regression Modeling: METHOD 2
#-------------------------------
library(glmnet)
#names(train.imp)
train <- train.imp[c('Id',boruta.selected.features,'SalePrice')]
#sapply(train, function(x){round(mean(x),2)})
#names(train)
# Remove outliers and Cook's Distance violators
train <- train[-c(305,1170,1299),]
train <- train[,-1] # Leave out the Id field
#names(train)
#n <- min(nrow(train), nrow(test))
n <- nrow(train)
set.seed(1-1)
train.set <- sample(1:n, 0.8*n)
#------------
# Parameters
#------------
library(caret)
params <- trainControl(method='repeatedcv',
number=10, repeats=5, verboseIter=TRUE)
#----------------------
# 1. Linear Regression
#----------------------
#----------------------
# 1. Linear Regression
#----------------------
set.seed(1-2)
lm <- train(SalePrice~.
+GarageType*GarageCars
+YearRemodAdd*YearBuilt
+GrLivArea*FullBath,
train[train.set,],
method='lm', trControl=params)
par(mfrow=c(2,2)); plot(lm$finalModel); par(mfrow=c(1,1))
lm.results <- lm$results
summary(lm)
saveRDS(lm, 'results/model.1.lm.rds')
#---------------------
# 2. Ridge Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(2-1)
ridge <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=0, lambda=grid))
saveRDS(ridge, 'results/model.2.ridge.rds')
ridge$bestTune
plot(ridge$finalModel, las=1, xvar='lambda', label=TRUE)
plot(ridge$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(ridge))
#---------------------
# 3. Lasso Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(3-1)
lasso <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=1, lambda=grid))
saveRDS(lasso, 'results/model.3.ridge.rds')
lasso$bestTune
plot(lasso$finalModel, las=1, xvar='lambda', label=TRUE)
plot(lasso$finalModel, las=1, xvar='dev',    label=TRUE)
#----------------
# 4. Elastic Net
#----------------
grid = 10^seq(5, -3, length = 10)
set.seed(4-1)
elastic <- train(SalePrice~., train[train.set,], method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=seq(0.001, 0.1, length = 10),
lambda=grid))
saveRDS(elastic, 'results/model.4.elastic.rds')
plot(elastic)
plot(elastic$finalModel, las=1, xvar='lambda', label=TRUE)
plot(elastic$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(elastic))
#----------------
# 4.1 Comparison
#----------------
library(caret)
model.list <- list(Linear=lm, Ridege=ridge, Lasso=lasso, Elastic=elastic)
comparison <- resamples(model.list)
saveRDS(summary(comparison), 'results/model.comparison.rds')
#bwplot(comparison)
xyplot(comparison, metric='RMSE')
#----------------
# 4.2 Best Model
#----------------
bestOne <- elastic$bestTune
saveRDS(elastic, 'model.4.elastic.bestOne.rds')
#---------------
# 5. PREDICTION
#---------------
predict.train <- predict(elastic, train[train.set,])
training <- sqrt(mean( (train[train.set,]$SalePrice - predict.train)^2) )
predict.test <- predict(elastic, train[-train.set,])
testing  <- sqrt(mean((train[-train.set,]$SalePrice - predict.test)^2))
cat('mse trianing = ', training, ', testing = ', testing)
source('yc.housing.rtm.test.prep.R')
test.tmp <- mice(test.prep, m=5, seed=111);
test.tmp <- complete(test.tmp,1);
test <- test.tmp[c('Id',boruta.selected.features)]
#test <- test.tmp[-insignificant.features]
#names(test)
#test <- as.matrix(as.data.frame(lapply(test, as.numeric)))
test.id <- test[1]
test <- test[-1]
prediction.lm      <- predict(lm     , test)
prediction.ridge   <- predict(ridge  , test)
prediction.lasso   <- predict(lasso  , test)
prediction.elastic <- predict(elastic, test)
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
prediction.elastic <- predict(elastic, test)
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
prediction.elastic <- predict(elastic, test)
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
library(glmnet)
library(plotly)
library(psych)
#------------------
# Data Preparation
#------------------
source('yc.housing.rtm.train.prep.R')
#names(train.prep)
#----------------------
# Imputation of Values
#----------------------
library(mice)
train.tmp <- mice(train.prep,  m=5, seed=111);
train.imp <- complete(train.tmp,3)
#sum(is.na(train.imp))
#names(train.imp)
write.csv(train.imp, 'data/yc.train.imp.csv')
# Boruta Selected Features from train.boruta.fix
boruta.selected.features = c(
"MSSubClass",
#"MSZoning",     "LotFrontage",
"LotArea",
"LandContour", "LandSlope",
#"Neighborhood",
#"Condition1",   "BldgType",
"HouseStyle",
#"OverallQual",
"OverallCond",  "YearBuilt",
"YearRemodAdd",
#"Exterior1st",
#"Exterior2nd",
#"MasVnrType",
"ExterQual", "ExterCond",
#"BsmtQual",
#"BsmtCond",
#"BsmtExposure",
#"BsmtFinType1",
"BsmtFinSF1",
#"BsmtUnfSF",
#"TotalBsmtSF",
#"HeatingQC", "CentralAir",
"X1stFlrSF",
#"X2ndFlrSF",
"GrLivArea", "FullBath",
#"HalfBath",
#"BedroomAbvGr", "KitchenAbvGr",
"KitchenQual",
#"TotRmsAbvGrd",
"Functional",   "Fireplaces",
"GarageType",
#"GarageYrBlt",
#"GarageFinish",
"GarageCars"
#"GarageArea",
#"GarageQual",   "GarageCond"
#"PavedDrive",
#"WoodDeckSF",   "OpenPorchSF"
)
#names(train.imp)
#-------------------------------
# Regression Modeling: METHOD 2
#-------------------------------
library(glmnet)
#names(train.imp)
train <- train.imp[c('Id',boruta.selected.features,'SalePrice')]
#sapply(train, function(x){round(mean(x),2)})
#names(train)
# Remove outliers and Cook's Distance violators
train <- train[-c(305,1170,1299),]
train <- train[,-1] # Leave out the Id field
#names(train)
#n <- min(nrow(train), nrow(test))
n <- nrow(train)
set.seed(1-1)
train.set <- sample(1:n, 0.8*n)
#------------
# Parameters
#------------
library(caret)
params <- trainControl(method='repeatedcv',
number=10, repeats=5, verboseIter=TRUE)
#----------------------
# 1. Linear Regression
#----------------------
set.seed(1-2)
lm <- train(SalePrice~.
+GarageType*GarageCars
+YearRemodAdd*YearBuilt
+GrLivArea*FullBath,
train[train.set,],
method='lm', trControl=params)
par(mfrow=c(2,2)); plot(lm$finalModel); par(mfrow=c(1,1))
lm.results <- lm$results
summary(lm)
saveRDS(lm, 'results/model.1.lm.rds')
#---------------------
# 2. Ridge Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(2-1)
ridge <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=0, lambda=grid))
saveRDS(ridge, 'results/model.2.ridge.rds')
ridge$bestTune
plot(ridge$finalModel, las=1, xvar='lambda', label=TRUE)
plot(ridge$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(ridge))
#---------------------
# 3. Lasso Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(3-1)
lasso <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=1, lambda=grid))
saveRDS(lasso, 'results/model.3.ridge.rds')
lasso$bestTune
plot(lasso$finalModel, las=1, xvar='lambda', label=TRUE)
plot(lasso$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(lasso))
#----------------
# 4. Elastic Net
#----------------
grid = 10^seq(5, -3, length = 10)
set.seed(4-1)
elastic <- train(SalePrice~., train[train.set,], method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=seq(0.001, 0.1, length = 10),
lambda=grid))
saveRDS(elastic, 'results/model.4.elastic.rds')
plot(elastic)
plot(elastic$finalModel, las=1, xvar='lambda', label=TRUE)
plot(elastic$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(elastic))
#----------------
# 4.1 Comparison
#----------------
library(caret)
model.list <- list(Linear=lm, Ridege=ridge, Lasso=lasso, Elastic=elastic)
comparison <- resamples(model.list)
saveRDS(summary(comparison), 'results/model.comparison.rds')
#bwplot(comparison)
xyplot(comparison, metric='RMSE')
# do a 3d plot here
#----------------
# 4.2 Best Model
#----------------
bestOne <- elastic$bestTune
#coef(bestOne, s=elastic$bestTune$lambda)
saveRDS(elastic, 'model.4.elastic.bestOne.rds')
#elastic <- readRDS('elastic.bestOne.rds')
#---------------
# 5. PREDICTION
#---------------
predict.train <- predict(elastic, train[train.set,])
training <- sqrt(mean( (train[train.set,]$SalePrice - predict.train)^2) )
predict.test <- predict(elastic, train[-train.set,])
testing  <- sqrt(mean((train[-train.set,]$SalePrice - predict.test)^2))
cat('mse trianing = ', training, ', testing = ', testing)
#plot(predict.train, las=1, cex.axis=0.7, main='Prediction by Training Data', xlab='Training Data')
#plot(predict.test,  las=1, cex.axis=0.7, main='Prediction by Testing Data' , xlab='Testing Data' )
#------------
source('yc.housing.rtm.test.prep.R')
test.tmp <- mice(test.prep, m=5, seed=111);
test.tmp <- complete(test.tmp,1);
#names(test.tmp)
test <- test.tmp[c('Id',boruta.selected.features)]
#test <- test.tmp[-insignificant.features]
#names(test)
#test <- as.matrix(as.data.frame(lapply(test, as.numeric)))
test.id <- test[1]
test <- test[-1]
prediction.lm      <- predict(lm     , test)
prediction.ridge   <- predict(ridge  , test)
prediction.lasso   <- predict(lasso  , test)
prediction.elastic <- predict(elastic, test)
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
#plot(prediction.lm, las=1, cex.axis=0.7, main='Prediction by lm' , xlab='Test Dataset' )
#plot(prediction.ridge, las=1, cex.axis=0.7, main='Prediction by ridge' , xlab='Test Dataset' )
#plot(prediction.lasso, las=1, cex.axis=0.7, main='Prediction by lasso' , xlab='Test Dataset' )
#plot(prediction.elastic, las=1, cex.axis=0.7, main='Prediction by elastic' , xlab='Test Dataset' )
prediction.elastic
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
library(glmnet)
library(plotly)
library(psych)
#------------------
# Data Preparation
#------------------
source('yc.housing.rtm.train.prep.R')
#names(train.prep)
#----------------------
# Imputation of Values
#----------------------
library(mice)
train.tmp <- mice(train.prep,  m=5, seed=111);
train.imp <- complete(train.tmp,3)
#sum(is.na(train.imp))
#names(train.imp)
write.csv(train.imp, 'data/yc.train.imp.csv')
# Boruta Selected Features from train.boruta.fix
boruta.selected.features = c(
"MSSubClass",
#"MSZoning",     "LotFrontage",
"LotArea",
"LandContour", "LandSlope",
#"Neighborhood",
#"Condition1",   "BldgType",
"HouseStyle",
#"OverallQual",
"OverallCond",  "YearBuilt",
"YearRemodAdd",
#"Exterior1st",
#"Exterior2nd",
#"MasVnrType",
"ExterQual", "ExterCond",
#"BsmtQual",
#"BsmtCond",
#"BsmtExposure",
#"BsmtFinType1",
"BsmtFinSF1",
#"BsmtUnfSF",
#"TotalBsmtSF",
#"HeatingQC", "CentralAir",
"X1stFlrSF",
#"X2ndFlrSF",
"GrLivArea", "FullBath",
#"HalfBath",
#"BedroomAbvGr", "KitchenAbvGr",
"KitchenQual",
#"TotRmsAbvGrd",
"Functional",   "Fireplaces",
"GarageType",
#"GarageYrBlt",
#"GarageFinish",
"GarageCars"
#"GarageArea",
#"GarageQual",   "GarageCond"
#"PavedDrive",
#"WoodDeckSF",   "OpenPorchSF"
)
#names(train.imp)
#-------------------------------
# Regression Modeling: METHOD 2
#-------------------------------
library(glmnet)
#names(train.imp)
train <- train.imp[c('Id',boruta.selected.features,'SalePrice')]
#sapply(train, function(x){round(mean(x),2)})
#names(train)
# Remove outliers and Cook's Distance violators
train <- train[-c(305,1170,1299),]
train <- train[,-1] # Leave out the Id field
#names(train)
#n <- min(nrow(train), nrow(test))
n <- nrow(train)
set.seed(1-1)
train.set <- sample(1:n, 0.8*n)
#------------
# Parameters
#------------
library(caret)
params <- trainControl(method='repeatedcv',
number=10, repeats=5, verboseIter=TRUE)
#----------------------
# 1. Linear Regression
#----------------------
set.seed(1-2)
lm <- train(SalePrice~.
+GarageType*GarageCars
+YearRemodAdd*YearBuilt
+GrLivArea*FullBath,
train[train.set,],
method='lm', trControl=params)
par(mfrow=c(2,2)); plot(lm$finalModel); par(mfrow=c(1,1))
lm.results <- lm$results
summary(lm)
saveRDS(lm, 'results/model.1.lm.rds')
#---------------------
# 2. Ridge Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(2-1)
ridge <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=0, lambda=grid))
saveRDS(ridge, 'results/model.2.ridge.rds')
ridge$bestTune
plot(ridge$finalModel, las=1, xvar='lambda', label=TRUE)
plot(ridge$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(ridge))
#---------------------
# 3. Lasso Regression
#---------------------
grid = 10^seq(5, -3, length = 100)
set.seed(3-1)
lasso <- train(SalePrice~., train[train.set,],
method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=1, lambda=grid))
saveRDS(lasso, 'results/model.3.ridge.rds')
lasso$bestTune
plot(lasso$finalModel, las=1, xvar='lambda', label=TRUE)
plot(lasso$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(lasso))
#----------------
# 4. Elastic Net
#----------------
grid = 10^seq(5, -3, length = 10)
set.seed(4-1)
elastic <- train(SalePrice~., train[train.set,], method='glmnet', trControl=params,
tuneGrid=expand.grid(alpha=seq(0.001, 0.1, length = 10),
lambda=grid))
saveRDS(elastic, 'results/model.4.elastic.rds')
plot(elastic)
plot(elastic$finalModel, las=1, xvar='lambda', label=TRUE)
plot(elastic$finalModel, las=1, xvar='dev',    label=TRUE)
plot(varImp(elastic))
#----------------
# 4.1 Comparison
#----------------
library(caret)
model.list <- list(Linear=lm, Ridege=ridge, Lasso=lasso, Elastic=elastic)
comparison <- resamples(model.list)
saveRDS(summary(comparison), 'results/model.comparison.rds')
#bwplot(comparison)
xyplot(comparison, metric='RMSE')
# do a 3d plot here
#----------------
# 4.2 Best Model
#----------------
bestOne <- elastic$bestTune
#coef(bestOne, s=elastic$bestTune$lambda)
saveRDS(elastic, 'model.4.elastic.bestOne.rds')
#elastic <- readRDS('elastic.bestOne.rds')
#---------------
# 5. PREDICTION
#---------------
predict.train <- predict(elastic, train[train.set,])
training <- sqrt(mean( (train[train.set,]$SalePrice - predict.train)^2) )
predict.test <- predict(elastic, train[-train.set,])
testing  <- sqrt(mean((train[-train.set,]$SalePrice - predict.test)^2))
cat('mse trianing = ', training, ', testing = ', testing)
#plot(predict.train, las=1, cex.axis=0.7, main='Prediction by Training Data', xlab='Training Data')
#plot(predict.test,  las=1, cex.axis=0.7, main='Prediction by Testing Data' , xlab='Testing Data' )
#------------
source('yc.housing.rtm.test.prep.R')
test.tmp <- mice(test.prep, m=5, seed=111);
test.tmp <- complete(test.tmp,1);
#names(test.tmp)
test <- test.tmp[c('Id',boruta.selected.features)]
#test <- test.tmp[-insignificant.features]
#names(test)
#test <- as.matrix(as.data.frame(lapply(test, as.numeric)))
test.id <- test[1]
test <- test[-1]
prediction.elastic <- predict(elastic, test)
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
write.csv(prediction.elastic, 'prediction.house.price.elastic.csv' )
write.csv(prediction.elastic, 'results/prediction.house.price.elastic.csv' )
